# -*- coding: utf-8 -*-
"""Part_2.8_special_groups_affected_by_evictions_normal_times (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LVKr9bZIir_RVW6BWD3wlScU44etMatS
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from scipy import stats
import datetime as dt
import matplotlib
import matplotlib.pyplot as plt
import os
import io
import geopandas as gpd
import seaborn as sns
# suppress warning
import warnings
warnings.filterwarnings('ignore')

# %matplotlib inline

pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)
pd.set_option('display.width', None)
# display all columns
# avoid scientific digit
# pd.options.display.float_format = '{:.2f}'.format

from google.colab import drive
drive.mount('/content/drive')

link1 = '/content/drive/My Drive/X999/bbl_evictions_311_svi_normal_times_2.7.csv'
link2 = '/content/drive/My Drive/X999/bbl_evictions_311_svi_covid_2.7.csv'
normal = pd.read_csv(link1)
covid = pd.read_csv(link2)

normal.columns, covid.columns, normal.shape, covid.shape

file_path3 = '/content/drive/My Drive/X999/bbl_cleaned.csv'
bbl_cleaned = pd.read_csv(file_path3)

bbl_cleaned.columns
# there is no nta in this dataset either, so it would be error-prone to approximate the nta data

normal[['nta', 'e_totpop', 'evictions_nta_per_1kunit_per_1kpop', 'average_year_eviction_nta_count', 'unit_per_nta']].head(10)

# 'evictions_nta_per_1kunit_per_1kpop'/('e_totpop' * 'unit_per_nta') *1000 *1000

# but this is still an inflated number,
# because for the buildings never appeared in the eviction dataset, their buildings and units were not counted at all
# we would have to have this limitation because bbl dataset did not have nta, and if we were to correctly include all buildings in
# each nta, we will need to derive the nta from community_board and census_tract, which will more likely to introduce errors and misjudgements
# than using only the buildings affected and were actually in the eviction dataset.

"""# **SVI measure analysis**

four types of aggregated and groupedby analysis:

average eviction count per building, per year;

average eviction count per nta population;

average eviction count per unit per building per year;

average eviction count per unit per nta population.

SVI measures:

ep_age17: age 17 or younger.
possible social causes impacted: homelessness, change of schools, education impact; \

ep_age65: age 65 and above.
possible social causes impacted: homelessness;\

ep_unemp: unemployed pct.
possible social causes impacted: homelessness;\
"""

analysis_columns = normal[['primary_key', 'bin', 'bbl', 'latitude', 'longitude', 'eviction_address', 'zipcode', 'borough', 'nta','average_year_eviction_count', 	'average_year_eviction_unit_count',
                              'average_year_eviction_nta_count',	'evictions_nta_per_1kpop', 'evictions_nta_per_1kunit_per_1kpop',
                              'unitsres', 'e_totpop']]
analysis_columns.head(1)

normal['ep_age17'].head(1)
# pct

"""# **Groupby and aggregate first**"""

svi_analysis_columns = ['ep_age17', 'ep_age65', 'ep_unemp', 'ep_afam', 'ep_hisp', 'evictions_nta_per_1kpop', 'evictions_nta_per_1kunit_per_1kpop',
                        'average_year_eviction_count', 'average_year_eviction_unit_count', 'average_year_eviction_nta_count']

"""## **Children**"""

normal[['nta', 'primary_key', 'eviction_address', 'average_year_eviction_nta_count']].sort_values('nta', ascending=True).head(10)

normal.groupby('nta')[svi_analysis_columns].mean().head(10)
# using mean() here, because the average_year_eviction_nta_count might be slightly different due to zipcode
# svi measures. Here, we just use mean to offset some tiny deviations and get the most accurate one as much as possible

# rate of children affected per 1000 people in the population in each nta
normal['children_impacted_nta_per_1kpop'] = (normal['evictions_nta_per_1kpop'] * (normal['ep_age17'] / 100))
# normal['children_affected_per_1k_2'] = ((normal['average_year_eviction_nta_count'] /normal['e_totpop'])*1000 * (normal['ep_age17'] / 100))
normal[['nta', 'children_impacted_nta_per_1kpop']].head(10)

"""## **Unemployment**"""

normal['unemployed_impacted_nta_per_1kpop'] = normal['evictions_nta_per_1kpop'] * normal['ep_unemp']/100
normal['unemployed_impacted_nta_per_1kpop'].head()

"""## **Elderly**"""

normal['elderly_impacted_nta_per_1kpop'] = normal['evictions_nta_per_1kpop'] * normal['ep_age65']/100
normal['elderly_impacted_nta_per_1kpop'].head()

"""## **black and hispanics impacted**"""

normal['bh_impacted_nta_per_1kpop'] = normal['evictions_nta_per_1kpop'] * (normal['ep_afam'] + normal['ep_hisp'])/100
normal['bh_impacted_nta_per_1kpop'].head()

"""# **housing burden areas impacted**

defintion: Housing cost-burdened occupied housing
units with annual income less than $75,000
(30%+ of income spent on housing costs)
estimate, 2018-2022 ACS
source: https://www.atsdr.cdc.gov/place-health/media/pdfs/2024/10/SVI2022Documentation.pdf

It is the pct of households that spend more tha 30% of income on housing costs.
"""

normal[['nta', 'ep_hburd']].sort_values('ep_hburd', ascending=True).head()

Albans = normal[normal['nta'] == 'St. Albans']

normal.loc[normal['nta'] == 'St. Albans', 'ep_hburd'] =  64.2
# https://anhd.org/report/how-affordable-housing-threatened-your-neighborhood-2020/
# https://www.nyc.gov/assets/doh/downloads/pdf/data/2018chp-qn12.pdf
normal.loc[normal['nta'] == 'East Elmhurst', 'ep_hburd'] = 54.2

normal['hburd_impacted_nta_per_1kpop'] = normal['evictions_nta_per_1kpop'] * normal['ep_hburd']/100
normal['hburd_impacted_nta_per_1kpop'].head()

# normal.drop(columns = ['children_impacted_nta_per_1k', 'unemployed_impacted_nta_per_1k', 'elderly_impacted_nta_per_1k'], inplace = True)

normal.columns[-15:], normal.shape

"""# **Analysis**

## **Children Analysis: The most vulnerable nta for kids (high likelihood for change of schools or dropping out)**
"""

svi_analysis_columns = ['children_impacted_nta_per_1kpop', 'elderly_impacted_nta_per_1kpop', 'unemployed_impacted_nta_per_1kpop', 'bh_impacted_nta_per_1kpop', 'hburd_impacted_nta_per_1kpop']

svi_analysis_df = normal.groupby('nta')[svi_analysis_columns].mean()
svi_analysis_df.head()

children_top_15 = svi_analysis_df.sort_values('children_impacted_nta_per_1kpop', ascending=False)['children_impacted_nta_per_1kpop'].head(15)
children_bottom_15 = svi_analysis_df.sort_values('children_impacted_nta_per_1kpop', ascending=True)['children_impacted_nta_per_1kpop'].head(15)
children_top_15

children_bottom_15

children_top_15 = children_top_15.to_frame()
children_bottom_15 = children_bottom_15.to_frame()
# series can't be transposed

children_top_15.T

children_bottom_15.T

"""## **Elderly analysis: The most vulnerable nta for elderly (more demand for social security/welfare)**"""

elderly_top_15 = svi_analysis_df.sort_values('elderly_impacted_nta_per_1kpop', ascending=False)['elderly_impacted_nta_per_1kpop'].head(15)
elderly_bottom_15 = svi_analysis_df.sort_values('elderly_impacted_nta_per_1kpop', ascending=True)['elderly_impacted_nta_per_1kpop'].head(15)
elderly_top_15 = elderly_top_15.to_frame()
elderly_bottom_15 = elderly_bottom_15.to_frame()

elderly_top_15.T

elderly_bottom_15.T

"""## **Unemployed analysis: The most vulnerable nta for elderly (more demand for social security/welfare, homelessness)**"""

unemployed_top_15 = svi_analysis_df.sort_values('unemployed_impacted_nta_per_1kpop', ascending=False)['unemployed_impacted_nta_per_1kpop'].head(15)
unemployed_bottom_15 = svi_analysis_df.sort_values('unemployed_impacted_nta_per_1kpop', ascending=True)['unemployed_impacted_nta_per_1kpop'].head(15)
unemployed_top_15 = unemployed_top_15.to_frame()
unemployed_bottom_15 = unemployed_bottom_15.to_frame()

unemployed_top_15.T

unemployed_bottom_15.T

"""## **Black + Hispanic analysis: The most vulnerable nta for black and hispanic population**"""

bh_top_15 = svi_analysis_df.sort_values('bh_impacted_nta_per_1kpop', ascending=False)['bh_impacted_nta_per_1kpop'].head(15)
bh_bottom_15 = svi_analysis_df.sort_values('bh_impacted_nta_per_1kpop', ascending=True)['bh_impacted_nta_per_1kpop'].head(15)
bh_top_15 = bh_top_15.to_frame()
bh_bottom_15 = bh_bottom_15.to_frame()

bh_top_15.T

bh_bottom_15.T



"""# **Housing Burden areas impacted by evictions**"""



housing_burden_top_15 = svi_analysis_df.sort_values('hburd_impacted_nta_per_1kpop', ascending=False)['hburd_impacted_nta_per_1kpop'].head(15)
housing_burden_bottom_15 = svi_analysis_df.sort_values('hburd_impacted_nta_per_1kpop', ascending=True)['hburd_impacted_nta_per_1kpop'].head(15)
housing_burden_top_15 = housing_burden_top_15.to_frame()
housing_burden_bottom_15 = housing_burden_bottom_15.to_frame()

housing_burden_top_15.T

housing_burden_bottom_15.T

# change some of the analysis columns' names so they are more obvious
# building based: 'average_year_eviction_count'
# 'average_year_eviction_unit_count',
# 'average_year_eviction_nta_count',
# 'evictions_nta_per_1k',
# 'evictions_nta_per_unit_per_1k',
normal.rename(columns={'average_year_eviction_count':'average_year_eviction_count(building)',
                       'average_year_eviction_unit_count':'average_year_eviction_count_per_unit(building)',
                      'average_year_eviction_nta_count':'average_year_eviction_count_per_nta(nta)',
                      'evictions_nta_per_1kpop':'evictions_per_nta_1kpop(nta)',
                      'evictions_nta_per_1kunit_per_1kpop':'evictions_per_1kunit_nta_1kpop(nta)'}, inplace=True)

normal.head()

normal['average_year_eviction_per_building_nta(nta)'] = normal['average_year_eviction_count_per_nta(nta)'] / normal['buildings_affected_per_nta'] * 10
normal['average_year_eviction_per_building_nta(nta)'].head()
# every 100 buildings in an nta, what are their average eviction counts

"""## **Interpretations**
- Measures eviction intensity per building.
- Answers the question:"In a particular nta, every 10 buildings, how many evictions do they have?"


$$
\text{average year eviction per building per neighborhood} = \left( \frac{\text{average year eviction count per nta}}{\text{buildings affected per nta}} \right) \times 10
$$

"""

normal[['nta', 'bin', 'buildings_affected_per_nta']]

normal[['nta', 'bin', 'e_totpop', 'buildings_affected_per_nta', 'unit_per_nta', 'average_year_eviction_count(building)', 'average_year_eviction_count_per_unit(building)', 'average_year_eviction_count_per_nta(nta)',
        'average_year_eviction_per_building_nta(nta)',\
        'evictions_per_nta_1kpop(nta)', 'evictions_per_1kunit_nta_1kpop(nta)']]

normal.columns[-13:]
# all analysis columns

# normal.drop(columns=['average_year_eviction_per_building_nta(nta)'], inplace=True)

# save the updated normal times data
normal.to_csv('/content/drive/My Drive/X999/bbl_evictions_311_svi_normal_times_svi.csv', index=False)

# normal.drop(columns=['hburd_nta_per_1k'], inplace=True)

normal.shape
# 104 is correct

